{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPyOpt\n",
      "  Downloading https://files.pythonhosted.org/packages/52/be/669d505416d7e465b2aef7df3b58d590f56468c4f7dc50c91fe91b8a78d9/GPyOpt-1.2.6.tar.gz (56kB)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\lyncta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from GPyOpt) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\lyncta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from GPyOpt) (1.3.1)\n",
      "Collecting GPy>=1.8 (from GPyOpt)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/b7/efa0f6347f75e7a2d94ee9d7d72d0ebe4c534dbd98b5b68d05d39fde2c01/GPy-1.10.0-cp37-cp37m-win_amd64.whl (1.4MB)\n",
      "Collecting paramz>=0.9.0 (from GPy>=1.8->GPyOpt)\n",
      "  Downloading https://files.pythonhosted.org/packages/d8/37/4abbeb78d30f20d3402887f46e6e9f3ef32034a9dea65d243654c82c8553/paramz-0.9.5.tar.gz (71kB)\n",
      "Requirement already satisfied: cython>=0.29 in c:\\users\\lyncta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from GPy>=1.8->GPyOpt) (0.29.13)\n",
      "Requirement already satisfied: six in c:\\users\\lyncta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from GPy>=1.8->GPyOpt) (1.12.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\lyncta\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt) (4.4.0)\n",
      "Building wheels for collected packages: GPyOpt, paramz\n",
      "  Building wheel for GPyOpt (setup.py): started\n",
      "  Building wheel for GPyOpt (setup.py): finished with status 'done'\n",
      "  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-cp37-none-any.whl size=83627 sha256=036ee0b4b5a02e42725ac55ae6d52d4580940baff1f9c223bc56590cdecc4566\n",
      "  Stored in directory: C:\\Users\\lyncta\\AppData\\Local\\pip\\Cache\\wheels\\b2\\00\\69\\cfa967a125cf25e66f644be6193ad6f0edf231147879ad714f\n",
      "  Building wheel for paramz (setup.py): started\n",
      "  Building wheel for paramz (setup.py): finished with status 'done'\n",
      "  Created wheel for paramz: filename=paramz-0.9.5-cp37-none-any.whl size=102557 sha256=9108f0905926d94d3882997502083319c5b989b1900ff2fd86f8baa5574fe06d\n",
      "  Stored in directory: C:\\Users\\lyncta\\AppData\\Local\\pip\\Cache\\wheels\\c8\\4a\\0e\\6e0dc85541825f991c431619e25b870d4b812c911214690cf8\n",
      "Successfully built GPyOpt paramz\n",
      "Installing collected packages: paramz, GPy, GPyOpt\n",
      "Successfully installed GPy-1.10.0 GPyOpt-1.2.6 paramz-0.9.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GPyOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import add\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "DEVICE = 'cpu' # 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class DQNAgent(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.reward = 0\n",
    "        self.gamma = 0.9\n",
    "        self.dataframe = pd.DataFrame()\n",
    "        self.short_memory = np.array([])\n",
    "        self.agent_target = 1\n",
    "        self.agent_predict = 0\n",
    "        self.learning_rate = params['learning_rate']        \n",
    "        self.epsilon = 1\n",
    "        self.actual = []\n",
    "        self.first_layer = params['first_layer_size']\n",
    "        self.second_layer = params['second_layer_size']\n",
    "        self.third_layer = params['third_layer_size']\n",
    "        self.memory = collections.deque(maxlen=params['memory_size'])\n",
    "        self.weights = params['weights_path']\n",
    "        self.load_weights = params['load_weights']\n",
    "        self.optimizer = None\n",
    "        self.network()\n",
    "          \n",
    "    def network(self):\n",
    "        # Layers\n",
    "        self.f1 = nn.Linear(11, self.first_layer)\n",
    "        self.f2 = nn.Linear(self.first_layer, self.second_layer)\n",
    "        self.f3 = nn.Linear(self.second_layer, self.third_layer)\n",
    "        self.f4 = nn.Linear(self.third_layer, 3)\n",
    "        # weights\n",
    "        if self.load_weights:\n",
    "            self.model = self.load_state_dict(torch.load(self.weights))\n",
    "            print(\"weights loaded\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = F.relu(self.f2(x))\n",
    "        x = F.relu(self.f3(x))\n",
    "        x = F.softmax(self.f4(x), dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def get_state(self, game, player, food):\n",
    "        \"\"\"\n",
    "        Return the state.\n",
    "        The state is a numpy array of 11 values, representing:\n",
    "            - Danger 1 OR 2 steps ahead\n",
    "            - Danger 1 OR 2 steps on the right\n",
    "            - Danger 1 OR 2 steps on the left\n",
    "            - Snake is moving left\n",
    "            - Snake is moving right\n",
    "            - Snake is moving up\n",
    "            - Snake is moving down\n",
    "            - The food is on the left\n",
    "            - The food is on the right\n",
    "            - The food is on the upper side\n",
    "            - The food is on the lower side      \n",
    "        \"\"\"\n",
    "        state = [\n",
    "            (player.x_change == 20 and player.y_change == 0 and ((list(map(add, player.position[-1], [20, 0])) in player.position) or\n",
    "            player.position[-1][0] + 20 >= (game.game_width - 20))) or (player.x_change == -20 and player.y_change == 0 and ((list(map(add, player.position[-1], [-20, 0])) in player.position) or\n",
    "            player.position[-1][0] - 20 < 20)) or (player.x_change == 0 and player.y_change == -20 and ((list(map(add, player.position[-1], [0, -20])) in player.position) or\n",
    "            player.position[-1][-1] - 20 < 20)) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add, player.position[-1], [0, 20])) in player.position) or\n",
    "            player.position[-1][-1] + 20 >= (game.game_height-20))),  # danger straight\n",
    "\n",
    "            (player.x_change == 0 and player.y_change == -20 and ((list(map(add,player.position[-1],[20, 0])) in player.position) or\n",
    "            player.position[ -1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],\n",
    "            [-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == -20 and player.y_change == 0 and ((list(map(\n",
    "            add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
    "            (list(map(add,player.position[-1],[0,20])) in player.position) or player.position[-1][\n",
    "             -1] + 20 >= (game.game_height-20))),  # danger right\n",
    "\n",
    "             (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],[20,0])) in player.position) or\n",
    "             player.position[-1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == -20 and ((list(map(\n",
    "             add, player.position[-1],[-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
    "            (list(map(add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (\n",
    "            player.x_change == -20 and player.y_change == 0 and ((list(map(add,player.position[-1],[0,20])) in player.position) or\n",
    "            player.position[-1][-1] + 20 >= (game.game_height-20))), #danger left\n",
    "\n",
    "\n",
    "            player.x_change == -20,  # move left\n",
    "            player.x_change == 20,  # move right\n",
    "            player.y_change == -20,  # move up\n",
    "            player.y_change == 20,  # move down\n",
    "            food.x_food < player.x,  # food left\n",
    "            food.x_food > player.x,  # food right\n",
    "            food.y_food < player.y,  # food up\n",
    "            food.y_food > player.y  # food down\n",
    "        ]\n",
    "\n",
    "        for i in range(len(state)):\n",
    "            if state[i]:\n",
    "                state[i]=1\n",
    "            else:\n",
    "                state[i]=0\n",
    "\n",
    "        return np.asarray(state)\n",
    "\n",
    "    def set_reward(self, player, crash):\n",
    "        \"\"\"\n",
    "        Return the reward.\n",
    "        The reward is:\n",
    "            -10 when Snake crashes. \n",
    "            +10 when Snake eats food\n",
    "            0 otherwise\n",
    "        \"\"\"\n",
    "        self.reward = 0\n",
    "        if crash:\n",
    "            self.reward = -10\n",
    "            return self.reward\n",
    "        if player.eaten:\n",
    "            self.reward = 10\n",
    "        return self.reward\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store the <state, action, reward, next_state, is_done> tuple in a \n",
    "        memory buffer for replay memory.\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_new(self, memory, batch_size):\n",
    "        \"\"\"\n",
    "        Replay memory.\n",
    "        \"\"\"\n",
    "        if len(memory) > batch_size:\n",
    "            minibatch = random.sample(memory, batch_size)\n",
    "        else:\n",
    "            minibatch = memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            self.train()\n",
    "            torch.set_grad_enabled(True)\n",
    "            target = reward\n",
    "            next_state_tensor = torch.tensor(np.expand_dims(next_state, 0), dtype=torch.float32).to(DEVICE)\n",
    "            state_tensor = torch.tensor(np.expand_dims(state, 0), dtype=torch.float32, requires_grad=True).to(DEVICE)\n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.forward(next_state_tensor)[0])\n",
    "            output = self.forward(state_tensor)\n",
    "            target_f = output.clone()\n",
    "            target_f[0][np.argmax(action)] = target\n",
    "            target_f.detach()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = F.mse_loss(output, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()            \n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Train the DQN agent on the <state, action, reward, next_state, is_done>\n",
    "        tuple at the current timestep.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        target = reward\n",
    "        next_state_tensor = torch.tensor(next_state.reshape((1, 11)), dtype=torch.float32).to(DEVICE)\n",
    "        state_tensor = torch.tensor(state.reshape((1, 11)), dtype=torch.float32, requires_grad=True).to(DEVICE)\n",
    "        if not done:\n",
    "            target = reward + self.gamma * torch.max(self.forward(next_state_tensor[0]))\n",
    "        output = self.forward(state_tensor)\n",
    "        target_f = output.clone()\n",
    "        target_f[0][np.argmax(action)] = target\n",
    "        target_f.detach()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(output, target_f)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GPyOpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-010a285a056c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mGPyOpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethods\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbayesOpt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'GPyOpt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pygame\n",
    "import argparse\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from DQN import DQNAgent\n",
    "from random import randint\n",
    "import random\n",
    "import statistics\n",
    "import torch.optim as optim\n",
    "import torch \n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "from bayesOpt import *\n",
    "import datetime\n",
    "import distutils.util\n",
    "DEVICE = 'cpu' # 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#################################\n",
    "#   Define parameters manually  #\n",
    "#################################\n",
    "def define_parameters():\n",
    "    params = dict()\n",
    "    # Neural Network\n",
    "    params['epsilon_decay_linear'] = 1/100\n",
    "    params['learning_rate'] = 0.00013629\n",
    "    params['first_layer_size'] = 200    # neurons in the first layer\n",
    "    params['second_layer_size'] = 20   # neurons in the second layer\n",
    "    params['third_layer_size'] = 50    # neurons in the third layer\n",
    "    params['episodes'] = 250          \n",
    "    params['memory_size'] = 2500\n",
    "    params['batch_size'] = 1000\n",
    "    # Settings\n",
    "    params['weights_path'] = 'weights/weights.h5'\n",
    "    params['train'] = False\n",
    "    params[\"test\"] = True\n",
    "    params['plot_score'] = True\n",
    "    params['log_path'] = 'logs/scores_' + str(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")) +'.txt'\n",
    "    return params\n",
    "\n",
    "\n",
    "class Game:\n",
    "    \"\"\" Initialize PyGAME \"\"\"\n",
    "    \n",
    "    def __init__(self, game_width, game_height):\n",
    "        pygame.display.set_caption('SnakeGen')\n",
    "        self.game_width = game_width\n",
    "        self.game_height = game_height\n",
    "        self.gameDisplay = pygame.display.set_mode((game_width, game_height + 60))\n",
    "        self.bg = pygame.image.load(\"img/background.png\")\n",
    "        self.crash = False\n",
    "        self.player = Player(self)\n",
    "        self.food = Food()\n",
    "        self.score = 0\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "    def __init__(self, game):\n",
    "        x = 0.45 * game.game_width\n",
    "        y = 0.5 * game.game_height\n",
    "        self.x = x - x % 20\n",
    "        self.y = y - y % 20\n",
    "        self.position = []\n",
    "        self.position.append([self.x, self.y])\n",
    "        self.food = 1\n",
    "        self.eaten = False\n",
    "        self.image = pygame.image.load('img/snakeBody.png')\n",
    "        self.x_change = 20\n",
    "        self.y_change = 0\n",
    "\n",
    "    def update_position(self, x, y):\n",
    "        if self.position[-1][0] != x or self.position[-1][1] != y:\n",
    "            if self.food > 1:\n",
    "                for i in range(0, self.food - 1):\n",
    "                    self.position[i][0], self.position[i][1] = self.position[i + 1]\n",
    "            self.position[-1][0] = x\n",
    "            self.position[-1][1] = y\n",
    "\n",
    "    def do_move(self, move, x, y, game, food, agent):\n",
    "        move_array = [self.x_change, self.y_change]\n",
    "\n",
    "        if self.eaten:\n",
    "            self.position.append([self.x, self.y])\n",
    "            self.eaten = False\n",
    "            self.food = self.food + 1\n",
    "        if np.array_equal(move, [1, 0, 0]):\n",
    "            move_array = self.x_change, self.y_change\n",
    "        elif np.array_equal(move, [0, 1, 0]) and self.y_change == 0:  # right - going horizontal\n",
    "            move_array = [0, self.x_change]\n",
    "        elif np.array_equal(move, [0, 1, 0]) and self.x_change == 0:  # right - going vertical\n",
    "            move_array = [-self.y_change, 0]\n",
    "        elif np.array_equal(move, [0, 0, 1]) and self.y_change == 0:  # left - going horizontal\n",
    "            move_array = [0, -self.x_change]\n",
    "        elif np.array_equal(move, [0, 0, 1]) and self.x_change == 0:  # left - going vertical\n",
    "            move_array = [self.y_change, 0]\n",
    "        self.x_change, self.y_change = move_array\n",
    "        self.x = x + self.x_change\n",
    "        self.y = y + self.y_change\n",
    "\n",
    "        if self.x < 20 or self.x > game.game_width - 40 \\\n",
    "                or self.y < 20 \\\n",
    "                or self.y > game.game_height - 40 \\\n",
    "                or [self.x, self.y] in self.position:\n",
    "            game.crash = True\n",
    "        eat(self, food, game)\n",
    "\n",
    "        self.update_position(self.x, self.y)\n",
    "\n",
    "    def display_player(self, x, y, food, game):\n",
    "        self.position[-1][0] = x\n",
    "        self.position[-1][1] = y\n",
    "\n",
    "        if game.crash == False:\n",
    "            for i in range(food):\n",
    "                x_temp, y_temp = self.position[len(self.position) - 1 - i]\n",
    "                game.gameDisplay.blit(self.image, (x_temp, y_temp))\n",
    "\n",
    "            update_screen()\n",
    "        else:\n",
    "            pygame.time.wait(300)\n",
    "\n",
    "\n",
    "class Food(object):\n",
    "    def __init__(self):\n",
    "        self.x_food = 240\n",
    "        self.y_food = 200\n",
    "        self.image = pygame.image.load('img/food2.png')\n",
    "\n",
    "    def food_coord(self, game, player):\n",
    "        x_rand = randint(20, game.game_width - 40)\n",
    "        self.x_food = x_rand - x_rand % 20\n",
    "        y_rand = randint(20, game.game_height - 40)\n",
    "        self.y_food = y_rand - y_rand % 20\n",
    "        if [self.x_food, self.y_food] not in player.position:\n",
    "            return self.x_food, self.y_food\n",
    "        else:\n",
    "            self.food_coord(game, player)\n",
    "\n",
    "    def display_food(self, x, y, game):\n",
    "        game.gameDisplay.blit(self.image, (x, y))\n",
    "        update_screen()\n",
    "\n",
    "\n",
    "def eat(player, food, game):\n",
    "    if player.x == food.x_food and player.y == food.y_food:\n",
    "        food.food_coord(game, player)\n",
    "        player.eaten = True\n",
    "        game.score = game.score + 1\n",
    "\n",
    "\n",
    "def get_record(score, record):\n",
    "    if score >= record:\n",
    "        return score\n",
    "    else:\n",
    "        return record\n",
    "\n",
    "\n",
    "def display_ui(game, score, record):\n",
    "    myfont = pygame.font.SysFont('Segoe UI', 20)\n",
    "    myfont_bold = pygame.font.SysFont('Segoe UI', 20, True)\n",
    "    text_score = myfont.render('SCORE: ', True, (0, 0, 0))\n",
    "    text_score_number = myfont.render(str(score), True, (0, 0, 0))\n",
    "    text_highest = myfont.render('HIGHEST SCORE: ', True, (0, 0, 0))\n",
    "    text_highest_number = myfont_bold.render(str(record), True, (0, 0, 0))\n",
    "    game.gameDisplay.blit(text_score, (45, 440))\n",
    "    game.gameDisplay.blit(text_score_number, (120, 440))\n",
    "    game.gameDisplay.blit(text_highest, (190, 440))\n",
    "    game.gameDisplay.blit(text_highest_number, (350, 440))\n",
    "    game.gameDisplay.blit(game.bg, (10, 10))\n",
    "\n",
    "\n",
    "def display(player, food, game, record):\n",
    "    game.gameDisplay.fill((255, 255, 255))\n",
    "    display_ui(game, game.score, record)\n",
    "    player.display_player(player.position[-1][0], player.position[-1][1], player.food, game)\n",
    "    food.display_food(food.x_food, food.y_food, game)\n",
    "\n",
    "\n",
    "def update_screen():\n",
    "    pygame.display.update()\n",
    "\n",
    "\n",
    "def initialize_game(player, game, food, agent, batch_size):\n",
    "    state_init1 = agent.get_state(game, player, food)  # [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
    "    action = [1, 0, 0]\n",
    "    player.do_move(action, player.x, player.y, game, food, agent)\n",
    "    state_init2 = agent.get_state(game, player, food)\n",
    "    reward1 = agent.set_reward(player, game.crash)\n",
    "    agent.remember(state_init1, action, reward1, state_init2, game.crash)\n",
    "    agent.replay_new(agent.memory, batch_size)\n",
    "\n",
    "\n",
    "def plot_seaborn(array_counter, array_score, train):\n",
    "    sns.set(color_codes=True, font_scale=1.5)\n",
    "    sns.set_style(\"white\")\n",
    "    plt.figure(figsize=(13,8))\n",
    "    fit_reg = False if train== False else True        \n",
    "    ax = sns.regplot(\n",
    "        np.array([array_counter])[0],\n",
    "        np.array([array_score])[0],\n",
    "        #color=\"#36688D\",\n",
    "        x_jitter=.1,\n",
    "        scatter_kws={\"color\": \"#36688D\"},\n",
    "        label='Data',\n",
    "        fit_reg = fit_reg,\n",
    "        line_kws={\"color\": \"#F49F05\"}\n",
    "    )\n",
    "    # Plot the average line\n",
    "    y_mean = [np.mean(array_score)]*len(array_counter)\n",
    "    ax.plot(array_counter,y_mean, label='Mean', linestyle='--')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set(xlabel='# games', ylabel='score')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_mean_stdev(array):\n",
    "    return statistics.mean(array), statistics.stdev(array)    \n",
    "\n",
    "\n",
    "def test(params):\n",
    "    params['load_weights'] = True\n",
    "    params['train'] = False\n",
    "    params[\"test\"] = False \n",
    "    score, mean, stdev = run(params)\n",
    "    return score, mean, stdev\n",
    "\n",
    "\n",
    "def run(params):\n",
    "    \"\"\"\n",
    "    Run the DQN algorithm, based on the parameters previously set.   \n",
    "    \"\"\"\n",
    "    pygame.init()\n",
    "    agent = DQNAgent(params)\n",
    "    agent = agent.to(DEVICE)\n",
    "    agent.optimizer = optim.Adam(agent.parameters(), weight_decay=0, lr=params['learning_rate'])\n",
    "    counter_games = 0\n",
    "    score_plot = []\n",
    "    counter_plot = []\n",
    "    record = 0\n",
    "    total_score = 0\n",
    "    while counter_games < params['episodes']:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "        # Initialize classes\n",
    "        game = Game(440, 440)\n",
    "        player1 = game.player\n",
    "        food1 = game.food\n",
    "\n",
    "        # Perform first move\n",
    "        initialize_game(player1, game, food1, agent, params['batch_size'])\n",
    "        if params['display']:\n",
    "            display(player1, food1, game, record)\n",
    "        \n",
    "        steps = 0       # steps since the last positive reward\n",
    "        while (not game.crash) and (steps < 100):\n",
    "            if not params['train']:\n",
    "                agent.epsilon = 0.01\n",
    "            else:\n",
    "                # agent.epsilon is set to give randomness to actions\n",
    "                agent.epsilon = 1 - (counter_games * params['epsilon_decay_linear'])\n",
    "\n",
    "            # get old state\n",
    "            state_old = agent.get_state(game, player1, food1)\n",
    "\n",
    "            # perform random actions based on agent.epsilon, or choose the action\n",
    "            if random.uniform(0, 1) < agent.epsilon:\n",
    "                final_move = np.eye(3)[randint(0,2)]\n",
    "            else:\n",
    "                # predict action based on the old state\n",
    "                with torch.no_grad():\n",
    "                    state_old_tensor = torch.tensor(state_old.reshape((1, 11)), dtype=torch.float32).to(DEVICE)\n",
    "                    prediction = agent(state_old_tensor)\n",
    "                    final_move = np.eye(3)[np.argmax(prediction.detach().cpu().numpy()[0])]\n",
    "\n",
    "            # perform new move and get new state\n",
    "            player1.do_move(final_move, player1.x, player1.y, game, food1, agent)\n",
    "            state_new = agent.get_state(game, player1, food1)\n",
    "\n",
    "            # set reward for the new state\n",
    "            reward = agent.set_reward(player1, game.crash)\n",
    "            \n",
    "            # if food is eaten, steps is set to 0\n",
    "            if reward > 0:\n",
    "                steps = 0\n",
    "                \n",
    "            if params['train']:\n",
    "                # train short memory base on the new action and state\n",
    "                agent.train_short_memory(state_old, final_move, reward, state_new, game.crash)\n",
    "                # store the new data into a long term memory\n",
    "                agent.remember(state_old, final_move, reward, state_new, game.crash)\n",
    "\n",
    "            record = get_record(game.score, record)\n",
    "            if params['display']:\n",
    "                display(player1, food1, game, record)\n",
    "                pygame.time.wait(params['speed'])\n",
    "            steps+=1\n",
    "        if params['train']:\n",
    "            agent.replay_new(agent.memory, params['batch_size'])\n",
    "        counter_games += 1\n",
    "        total_score += game.score\n",
    "        print(f'Game {counter_games}      Score: {game.score}')\n",
    "        score_plot.append(game.score)\n",
    "        counter_plot.append(counter_games)\n",
    "    mean, stdev = get_mean_stdev(score_plot)\n",
    "    if params['train']:\n",
    "        model_weights = agent.state_dict()\n",
    "        torch.save(model_weights, params[\"weights_path\"])\n",
    "    if params['plot_score']:\n",
    "        plot_seaborn(counter_plot, score_plot, params['train'])\n",
    "    return total_score, mean, stdev\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set options to activate or deactivate the game view, and its speed\n",
    "    pygame.font.init()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    params = define_parameters()\n",
    "    parser.add_argument(\"--display\", nargs='?', type=distutils.util.strtobool, default=True)\n",
    "    parser.add_argument(\"--speed\", nargs='?', type=int, default=50)\n",
    "    parser.add_argument(\"--bayesianopt\", nargs='?', type=distutils.util.strtobool, default=False)\n",
    "    args = parser.parse_args()\n",
    "    print(\"Args\", args)\n",
    "    params['display'] = args.display\n",
    "    params['speed'] = args.speed\n",
    "    if args.bayesianopt:\n",
    "        bayesOpt = BayesianOptimizer(params)\n",
    "        bayesOpt.optimize_RL()\n",
    "    if params['train']:\n",
    "        print(\"Training...\")\n",
    "        params['load_weights'] = False   # when training, the network is not pre-trained\n",
    "        run(params)\n",
    "    if params['test']:\n",
    "        print(\"Testing...\")\n",
    "        params['train'] = False\n",
    "        params['load_weights'] = True\n",
    "        run(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
